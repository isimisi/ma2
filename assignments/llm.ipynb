{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm\n",
    "from decouple import config\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 2),)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 1e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape, # train_df.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = config('WX_API_KEY')\n",
    "\n",
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials=credentials, \n",
    "    project_id=\"41afd1e1-f579-4ce0-bf97-2c29ed058f5c\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=10,          # Maximum number of tokens to generate\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # We could also try a larger model!\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Business, Example: Ford: Monthly Sales Drop, Company Looks To New Vehicles Cruising along the ever-stretching road of decline. Auto giant Ford Motor (nyse: F - news - people ) reported vehicle sales in October that fell 5 from a year ago.\n",
      "Category: Sci/Tech, Example: China Closes 1,600 Internet Cafes in Crackdown China shut 1,600 Internet cafes between February and August and imposed \\$12.1 million worth of fines for allowing children to play violent or adult-only games and other violations, state media said.\n",
      "Category: Sports, Example: Agassi Overcomes Verdasco Power  STOCKHOLM (Reuters) - Andre Agassi marched into the  Stockholm Open semifinals Friday, beating Spanish eighth seed  Fernando Verdasco 7-6, 6-2 in his toughest match of the  tournament.\n",
      "Category: World, Example: Large Explosion Heard in Central Baghdad (Reuters) Reuters - A large blast was heard in central\\Baghdad on Thursday, witnesses said.\n",
      "Remaining dataset size: 756\n"
     ]
    }
   ],
   "source": [
    "# using for example\n",
    "\n",
    "categories = test_df[\"label\"].unique()\n",
    "\n",
    "examples = {}\n",
    "indices_to_remove = []\n",
    "for i, row in test_df.iterrows():\n",
    "    if row[\"label\"] in categories and row[\"label\"] not in examples:\n",
    "        examples[row[\"label\"]] = row[\"text\"]\n",
    "        indices_to_remove.append(i)\n",
    "    if len(examples) == len(categories):\n",
    "        break\n",
    "\n",
    "# Print examples\n",
    "for category, text in examples.items():\n",
    "    print(f\"Category: {category}, Example: {text}\")\n",
    "\n",
    "# Remove selected examples to avoid data leakage\n",
    "test_df_without_examples = test_df.drop(indices_to_remove).reset_index(drop=True)\n",
    "\n",
    "# Print remaining dataset size\n",
    "print(\"Remaining dataset size:\", len(test_df_without_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Classify the following news story into one of the four categories: \n",
    "{categories}\n",
    "\n",
    "Text:  \n",
    "{text}  \n",
    "\n",
    "Respond with only the correct category name and nothing else.  \n",
    "\n",
    "Category:  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [03:50<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # format the prompt with the categories and the text\n",
    "    prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n",
    "    \n",
    "    # generate the response from the model\n",
    "    response = model.generate(prompt)\n",
    "\n",
    "    # extract the generated text from the response\n",
    "    prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "    # append the prediction to the list of predictions\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Business       0.52      0.93      0.67       190\n",
      "        Health       0.00      0.00      0.00         0\n",
      "          Java       0.00      0.00      0.00         0\n",
      " Miscellaneous       0.00      0.00      0.00         0\n",
      "      Sci/Tech       0.86      0.35      0.50       190\n",
      "         Space       0.00      0.00      0.00         0\n",
      "Space Sci/Tech       0.00      0.00      0.00         0\n",
      "        Sports       0.91      0.91      0.91       190\n",
      "         World       0.85      0.66      0.74       190\n",
      "\n",
      "      accuracy                           0.71       760\n",
      "     macro avg       0.35      0.32      0.31       760\n",
      "  weighted avg       0.78      0.71      0.70       760\n",
      "\n",
      "- Business\n",
      "- Sci/Tech\n",
      "- Sports\n",
      "- World\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df.label, predictions))\n",
    "print(CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I tried using zero-shot prompting, which gave similar results as the llm_guide\n",
    "\n",
    "Then I tried to use few-shot prompting, by using the data set to extract out 3 examples and remove them from the data set. The prompt I used look like the following:\n",
    "\n",
    "```py\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Your task is to classify news stories into one of four categories\n",
    "\n",
    "CATEGORIES:  \n",
    "{categories}  \n",
    "\n",
    "Ford: Monthly Sales Drop, Company Looks To New Vehicles Cruising along the ever-stretching road of decline. Auto giant Ford Motor (nyse: F - news - people ) reported vehicle sales in October that fell 5 from a year ago // Business\n",
    "\n",
    "United #39;s pension dilemma United Airlines says it likely will end funding for employee pension plans, a move that would be the largest ever default by a US company and could lead to a taxpayer-funded bailout rivaling the savings-and-loan fiasco of the 1980s // Business\n",
    "\n",
    "Comcast part of group wanting to buy MGM A consortium led by Sony Corp. of America that includes Comcast Corp. has entered into a definitive agreement to acquire Metro-Goldwyn Mayer Inc // Business\n",
    "\n",
    "{text}:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "This gave an accuract of 80%, but the LLM created a lot more categories than the 5, so I tried to tweak the examples to use:\n",
    "\n",
    "text: {text}\n",
    "\n",
    "category: {category}\n",
    "\n",
    "However, I couldn't get the LLM to only respond with one of the provided categories, even when prompting it to. So I tried to change the examples, so I'd have an example for each category. \n",
    "\n",
    "This didn't help at all. Instead I opted to use zero-shot prompting.\n",
    "\n",
    "Using an LLM for classification gave good accuracy but struggled with sticking to only the four categories, often creating new ones. BoW with logistic regression worked fine for strict categorization but had lower accuracy. BERT with logistic regression performed the best, offering better accuracy and understanding of context while keeping the categories intact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
